{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Encoder Decoder\n",
    "\n",
    "<b>reference</b></br>\n",
    "https://github.com/KDD-OpenSource/DeepADoTS<br/>\n",
    "https://stackoverflow.com/questions/42415909/initializing-lstm-hidden-state-tensorflow-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1029 entries, 0 to 1028\n",
      "Data columns (total 14 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   p (mbar)         1029 non-null   float64\n",
      " 1   T (degC)         1029 non-null   float64\n",
      " 2   Tpot (K)         1029 non-null   float64\n",
      " 3   Tdew (degC)      1029 non-null   float64\n",
      " 4   rh (%)           1029 non-null   float64\n",
      " 5   VPmax (mbar)     1029 non-null   float64\n",
      " 6   VPact (mbar)     1029 non-null   float64\n",
      " 7   VPdef (mbar)     1029 non-null   float64\n",
      " 8   sh (g/kg)        1029 non-null   float64\n",
      " 9   H2OC (mmol/mol)  1029 non-null   float64\n",
      " 10  rho (g/m**3)     1029 non-null   float64\n",
      " 11  wv (m/s)         1029 non-null   float64\n",
      " 12  max. wv (m/s)    1029 non-null   float64\n",
      " 13  wd (deg)         1029 non-null   float64\n",
      "dtypes: float64(14)\n",
      "memory usage: 112.7 KB\n"
     ]
    }
   ],
   "source": [
    "# data probing\n",
    "scaler = MinMaxScaler()\n",
    "df = pd.read_csv('jena_climate_2009_2016.csv').head(1029)\n",
    "df_train = df.drop(columns=['Date Time'])\n",
    "df_train[df_train.columns.values] = scaler.fit_transform(df_train[df_train.columns.values])\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training input shape:  (1000, 30, 14)\n",
      "Model: \"model_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_45 (InputLayer)       [(None, 30, 14)]          0         \n",
      "                                                                 \n",
      " lstmed_42 (LSTMED)          (20, 30, 14)              41358     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 41,358\n",
      "Trainable params: 41,358\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# * 探索代码\n",
    "batch_size = 20\n",
    "TIME_STEPS = 30\n",
    "\n",
    "# Generated training sequences for use in the model.\n",
    "def create_sequences(values, time_steps=TIME_STEPS):\n",
    "    output = []\n",
    "    for i in range(len(values) - time_steps + 1):\n",
    "        output.append(values[i : (i + time_steps)])\n",
    "    return np.stack(output)\n",
    "\n",
    "x_train = create_sequences(df_train.values)\n",
    "print(\"Training input shape: \", x_train.shape)\n",
    "\n",
    "inputs = Input(shape=(x_train.shape[1], x_train.shape[2]))\n",
    "outputs = LSTMED(units=64, batch_size=batch_size, training=True)(inputs)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss=\"mse\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "45/45 [==============================] - 45s 255ms/step - loss: 0.0942 - val_loss: 0.0200\n",
      "Epoch 2/10\n",
      "45/45 [==============================] - 1s 31ms/step - loss: 0.0196 - val_loss: 0.0086\n",
      "Epoch 3/10\n",
      "45/45 [==============================] - 1s 30ms/step - loss: 0.0117 - val_loss: 0.0063\n",
      "Epoch 4/10\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 0.0081 - val_loss: 0.0057\n",
      "Epoch 5/10\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 0.0059 - val_loss: 0.0047\n",
      "Epoch 6/10\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 0.0050 - val_loss: 0.0042\n",
      "Epoch 7/10\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 0.0047 - val_loss: 0.0041\n",
      "Epoch 8/10\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 0.0045 - val_loss: 0.0039\n",
      "Epoch 9/10\n",
      "45/45 [==============================] - 1s 28ms/step - loss: 0.0044 - val_loss: 0.0037\n",
      "Epoch 10/10\n",
      "45/45 [==============================] - 1s 27ms/step - loss: 0.0043 - val_loss: 0.0034\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "    tensorflow使用静态图模型，对lstm encoder decoder的支持存在下面的问题：\n",
    "\n",
    "    需要满足 训练数据集行数(1029) - TIME_STEPS + 1 能够整除 batch_size / validation_split\n",
    "    考虑使用 Sequence 接口解决这个问题.\n",
    "'''\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    x_train,\n",
    "    epochs=10,\n",
    "    batch_size=batch_size,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Repo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlow版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense, LSTM, Layer, Input\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class LSTMED(Layer):\n",
    "    '''\n",
    "        @param units number of hidden units\n",
    "    '''\n",
    "    def __init__(self, units, batch_size, training=False):\n",
    "        super(LSTMED, self).__init__()\n",
    "        self.units = units\n",
    "        self.batch_size = batch_size\n",
    "        self.training = training\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        time_steps = input_shape[1]\n",
    "        n_features = input_shape[-1]\n",
    "        # multi step lstm\n",
    "        self.encoder = LSTM(self.units, return_state=True, name=\"encoder\", kernel_initializer='Zeros', recurrent_initializer='Zeros', input_shape=(time_steps, n_features))\n",
    "        # single step lstm\n",
    "        self.decoder = LSTM(self.units, return_state=True, name=\"decoder\", kernel_initializer='Zeros', recurrent_initializer='Zeros', input_shape=(1, n_features))\n",
    "        self.hidden2output = Dense(n_features)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        batch_size = self.batch_size\n",
    "        h_0 = tf.zeros(shape=(batch_size, self.units))\n",
    "        c_0 = tf.zeros(shape=(batch_size, self.units))\n",
    "        # 1. Encode the timeseries to make use of the last hidden state.\n",
    "        _, h_0, c_0 = self.encoder(inputs, initial_state=[h_0, c_0])  # .float() here or .double() for the model\n",
    "\n",
    "        # 2. Use hidden state as initialization for our Decoder-LSTM\n",
    "\n",
    "        # 3. Also, use this hidden state to get the first output aka the last point of the reconstructed timeseries\n",
    "        # 4. Reconstruct timeseries backwards\n",
    "        #    * Use true data for training decoder\n",
    "        #    * Use hidden2output for prediction\n",
    "        output = tf.unstack(tf.transpose(tf.zeros_like(inputs), perm=[1,0,2]))\n",
    "        for i in reversed(range(inputs.shape[1])):\n",
    "            output[i] = self.hidden2output(h_0)\n",
    "\n",
    "            if self.training:\n",
    "                _, h_0, c_0 = self.decoder(tf.expand_dims(inputs[:, i], axis=1), initial_state=[h_0, c_0])\n",
    "            else:\n",
    "                _, h_0, c_0 = self.decoder(tf.expand_dims(output[:, i], axis=1), initial_state=[h_0, c_0])\n",
    "\n",
    "        return tf.transpose(tf.stack(output), perm=[1,0,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 启动代码\n",
    "df_train = pd.DataFrame(df.iloc[:, 4].values.reshape(-1, 1))\n",
    "model = LSTMED()\n",
    "model.fit(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.stats import multivariate_normal\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from tqdm import trange\n",
    "\n",
    "class Algorithm(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, module_name, name, seed, details=False):\n",
    "        self.logger = logging.getLogger(module_name)\n",
    "        self.name = name\n",
    "        self.seed = seed\n",
    "        self.details = details\n",
    "        self.prediction_details = {}\n",
    "\n",
    "        if self.seed is not None:\n",
    "            random.seed(seed)\n",
    "            np.random.seed(seed)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Train the algorithm on the given dataset\n",
    "        \"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        :return anomaly score\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class PyTorchUtils(metaclass=abc.ABCMeta):\n",
    "    def __init__(self, seed, gpu):\n",
    "        self.gpu = gpu\n",
    "        self.seed = seed\n",
    "        if self.seed is not None:\n",
    "            torch.manual_seed(self.seed)\n",
    "            torch.cuda.manual_seed(self.seed)\n",
    "        self.framework = 0\n",
    "\n",
    "    @property\n",
    "    def device(self):\n",
    "        return torch.device(f'cuda:{self.gpu}' if torch.cuda.is_available() and self.gpu is not None else 'cpu')\n",
    "\n",
    "    def to_var(self, t, **kwargs):\n",
    "        # ToDo: check whether cuda Variable.\n",
    "        t = t.to(self.device)\n",
    "        return Variable(t, **kwargs)\n",
    "\n",
    "    def to_device(self, model):\n",
    "        model.to(self.device)\n",
    "\n",
    "class LSTMED(Algorithm, PyTorchUtils):\n",
    "    def __init__(self, name: str = 'LSTM-ED', num_epochs: int = 10, batch_size: int = 20, lr: float = 1e-3,\n",
    "                 hidden_size: int = 5, sequence_length: int = 30, train_gaussian_percentage: float = 0.25,\n",
    "                 n_layers: tuple = (1, 1), use_bias: tuple = (True, True), dropout: tuple = (0, 0),\n",
    "                 seed: int = None, gpu: int = None, details=True):\n",
    "        Algorithm.__init__(self, __name__, name, seed, details=details)\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "\n",
    "        self.hidden_size = hidden_size\n",
    "        self.sequence_length = sequence_length\n",
    "        self.train_gaussian_percentage = train_gaussian_percentage\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.lstmed = None\n",
    "        self.mean, self.cov = None, None\n",
    "\n",
    "    def fit(self, X: pd.DataFrame):\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        indices = np.random.permutation(len(sequences))\n",
    "        split_point = int(self.train_gaussian_percentage * len(sequences))\n",
    "        train_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                  sampler=SubsetRandomSampler(indices[:-split_point]), pin_memory=True)\n",
    "        train_gaussian_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, drop_last=True,\n",
    "                                           sampler=SubsetRandomSampler(indices[-split_point:]), pin_memory=True)\n",
    "\n",
    "        self.lstmed = LSTMEDModule(X.shape[1], self.hidden_size,\n",
    "                                   self.n_layers, self.use_bias, self.dropout,\n",
    "                                   seed=self.seed, gpu=self.gpu)\n",
    "        self.to_device(self.lstmed)\n",
    "        optimizer = torch.optim.Adam(self.lstmed.parameters(), lr=self.lr)\n",
    "\n",
    "        self.lstmed.train()\n",
    "        for epoch in trange(self.num_epochs):\n",
    "            logging.debug(f'Epoch {epoch+1}/{self.num_epochs}.')\n",
    "            for ts_batch in train_loader:\n",
    "                output = self.lstmed(self.to_var(ts_batch))\n",
    "                loss = nn.MSELoss(size_average=False)(output, self.to_var(ts_batch.float()))\n",
    "                self.lstmed.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        self.lstmed.eval()\n",
    "        error_vectors = []\n",
    "        for ts_batch in train_gaussian_loader:\n",
    "            output = self.lstmed(self.to_var(ts_batch))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts_batch.float()))\n",
    "            error_vectors += list(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "\n",
    "        self.mean = np.mean(error_vectors, axis=0)\n",
    "        self.cov = np.cov(error_vectors, rowvar=False)\n",
    "\n",
    "    def predict(self, X: pd.DataFrame):\n",
    "        X.interpolate(inplace=True)\n",
    "        X.bfill(inplace=True)\n",
    "        data = X.values\n",
    "        sequences = [data[i:i + self.sequence_length] for i in range(data.shape[0] - self.sequence_length + 1)]\n",
    "        data_loader = DataLoader(dataset=sequences, batch_size=self.batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "        self.lstmed.eval()\n",
    "        mvnormal = multivariate_normal(self.mean, self.cov, allow_singular=True)\n",
    "        scores = []\n",
    "        outputs = []\n",
    "        errors = []\n",
    "        for idx, ts in enumerate(data_loader):\n",
    "            output = self.lstmed(self.to_var(ts))\n",
    "            error = nn.L1Loss(reduce=False)(output, self.to_var(ts.float()))\n",
    "            score = -mvnormal.logpdf(error.view(-1, X.shape[1]).data.cpu().numpy())\n",
    "            scores.append(score.reshape(ts.size(0), self.sequence_length))\n",
    "            if self.details:\n",
    "                outputs.append(output.data.numpy())\n",
    "                errors.append(error.data.numpy())\n",
    "\n",
    "        # stores seq_len-many scores per timestamp and averages them\n",
    "        scores = np.concatenate(scores)\n",
    "        lattice = np.full((self.sequence_length, data.shape[0]), np.nan)\n",
    "        for i, score in enumerate(scores):\n",
    "            lattice[i % self.sequence_length, i:i + self.sequence_length] = score\n",
    "        scores = np.nanmean(lattice, axis=0)\n",
    "\n",
    "        if self.details:\n",
    "            outputs = np.concatenate(outputs)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, output in enumerate(outputs):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = output\n",
    "            self.prediction_details.update({'reconstructions_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "            errors = np.concatenate(errors)\n",
    "            lattice = np.full((self.sequence_length, X.shape[0], X.shape[1]), np.nan)\n",
    "            for i, error in enumerate(errors):\n",
    "                lattice[i % self.sequence_length, i:i + self.sequence_length, :] = error\n",
    "            self.prediction_details.update({'errors_mean': np.nanmean(lattice, axis=0).T})\n",
    "\n",
    "        return scores\n",
    "\n",
    "\n",
    "class LSTMEDModule(nn.Module, PyTorchUtils):\n",
    "    def __init__(self, n_features: int, hidden_size: int,\n",
    "                 n_layers: tuple, use_bias: tuple, dropout: tuple,\n",
    "                 seed: int, gpu: int):\n",
    "        super().__init__()\n",
    "        PyTorchUtils.__init__(self, seed, gpu)\n",
    "        self.n_features = n_features\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.n_layers = n_layers\n",
    "        self.use_bias = use_bias\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.encoder = nn.LSTM(self.n_features, self.hidden_size, batch_first=True,\n",
    "                               num_layers=self.n_layers[0], bias=self.use_bias[0], dropout=self.dropout[0])\n",
    "        self.to_device(self.encoder)\n",
    "        self.decoder = nn.LSTM(self.n_features, self.hidden_size, batch_first=True,\n",
    "                               num_layers=self.n_layers[1], bias=self.use_bias[1], dropout=self.dropout[1])\n",
    "        self.to_device(self.decoder)\n",
    "        self.hidden2output = nn.Linear(self.hidden_size, self.n_features)\n",
    "        self.to_device(self.hidden2output)\n",
    "\n",
    "    def _init_hidden(self, batch_size):\n",
    "        return (self.to_var(torch.Tensor(self.n_layers[0], batch_size, self.hidden_size).zero_()),\n",
    "                self.to_var(torch.Tensor(self.n_layers[0], batch_size, self.hidden_size).zero_()))\n",
    "\n",
    "    def forward(self, ts_batch, return_latent: bool = False):\n",
    "        batch_size = ts_batch.shape[0]\n",
    "\n",
    "        # 1. Encode the timeseries to make use of the last hidden state.\n",
    "        enc_hidden = self._init_hidden(batch_size)  # initialization with zero\n",
    "        _, enc_hidden = self.encoder(ts_batch.float(), enc_hidden)  # .float() here or .double() for the model\n",
    "\n",
    "        # 2. Use hidden state as initialization for our Decoder-LSTM\n",
    "        dec_hidden = enc_hidden\n",
    "\n",
    "        # 3. Also, use this hidden state to get the first output aka the last point of the reconstructed timeseries\n",
    "        # 4. Reconstruct timeseries backwards\n",
    "        #    * Use true data for training decoder\n",
    "        #    * Use hidden2output for prediction\n",
    "        output = self.to_var(torch.Tensor(ts_batch.size()).zero_())\n",
    "        for i in reversed(range(ts_batch.shape[1])):\n",
    "            output[:, i, :] = self.hidden2output(dec_hidden[0][0, :])\n",
    "\n",
    "            if self.training:\n",
    "                _, dec_hidden = self.decoder(ts_batch[:, i].unsqueeze(1).float(), dec_hidden)\n",
    "            else:\n",
    "                _, dec_hidden = self.decoder(output[:, i].unsqueeze(1), dec_hidden)\n",
    "\n",
    "        return (output, enc_hidden[1][-1]) if return_latent else output"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "13f244bc7c79d3f70ce79b114c81c7859949da3dd8322c40885710ecc8043591"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
